# Modo de ejecución:
# - api   => usa LLM real (OpenAI/Gemini)
# - rules => usa generador local por reglas (sin API)
LLM_MODE=api

# Política de fallback cuando LLM_MODE=api:
# - raise => lanza error (comportamiento profesional para detectar fallos reales)
# - rules => cae al generador local si hay error de cuota/modelo/key
FALLBACK_MODE=raise

# Modelo principal (CrewAI/LiteLLM format)
# Ejemplos:
# MODEL=gpt-4o-mini
# MODEL=gemini/gemini-2.0-flash
# MODEL=anthropic/claude-3-5-haiku-latest
# MODEL=github/gpt-4.1-mini
MODEL=gpt-4o-mini

# Creatividad del modelo
TEMPERATURE=0.5

# Opción avanzada (agnóstica):
# Si defines LLM_API_KEY, se usa esta key sin importar proveedor.
# Si defines LLM_BASE_URL, se usa endpoint compatible OpenAI (útil para GitHub Models/OpenRouter/etc.)
LLM_API_KEY=
LLM_BASE_URL=

# Configura según proveedor que uses
OPENAI_API_KEY=
OPENAI_BASE_URL=
GOOGLE_API_KEY=
GEMINI_API_KEY=
ANTHROPIC_API_KEY=
GITHUB_TOKEN=
